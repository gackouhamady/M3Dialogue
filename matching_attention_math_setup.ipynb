{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# MatchingAttention: Mathematical Setup\n",
    "\n",
    "This module implements a matching attention mechanism, which computes attention weights between a candidate vector and a memory matrix, then returns an attention-pooled representation.\n",
    "\n",
    "## Notation\n",
    "Let \\( M \\in \\mathbb{R}^{L \times d_m} \\) be the memory matrix with sequence length \\( L \\) and memory dimension \\( d_m \\).\n",
    "\n",
    "Let \\( x \\in \\mathbb{R}^{d_c} \\) be the candidate vector with dimension \\( d_c \\).\n",
    "\n",
    "Let \\( \text{mask} \\in \\{0,1\\}^L \\) be an optional binary mask for the memory positions.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "### Linear Transformation:\n",
    "First, the candidate vector \\( x \\) is transformed to match the memory dimension:\n",
    "\n",
    "\\[\n",
    "x' = W x + b\n",
    "\\]\n",
    "\n",
    "where \\( W \\in \\mathbb{R}^{d_m \times d_c} \\), \\( b \\in \\mathbb{R}^{d_m} \\).\n",
    "\n",
    "This is implemented by `self.transform = nn.Linear(cand_dim, mem_dim, bias=True)`.\n",
    "\n",
    "### Attention Computation:\n",
    "The attention scores \\( \u0007lpha \\) are computed as:\n",
    "\n",
    "\\[\n",
    "\u0007lpha' = \tanh(x'^\top M) \\odot \text{mask}\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "\u0007lpha = \text{softmax}(\u0007lpha', \text{dim}=1)\n",
    "\\]\n",
    "\n",
    "where \\( \\odot \\) denotes element-wise multiplication and the mask is broadcast appropriately.\n",
    "\n",
    "### Normalization:\n",
    "The attention weights are normalized to sum to 1 over the valid (unmasked) positions:\n",
    "\n",
    "\\[\n",
    "\u0007lpha_{\text{norm}} = \frac{\u0007lpha \\odot \text{mask}}{\\sum (\u0007lpha \\odot \text{mask})}\n",
    "\\]\n",
    "\n",
    "### Attention Pooling:\n",
    "The final attended representation is computed as:\n",
    "\n",
    "\\[\n",
    "h = \\sum_{i=1}^{L} \u0007lpha_i M_i\n",
    "\\]\n",
    "\n",
    "This is implemented efficiently as a batched matrix multiplication.\n",
    "\n",
    "## Forward Pass\n",
    "\n",
    "The forward pass implements the following computation:\n",
    "\n",
    "```python\n",
    "# Input: M (memory), x (candidate), mask (optional)\n",
    "\n",
    "1. M_ = M.permute(1, 2, 0)  # Rearrange memory for matmul\n",
    "2. x_ = transform(x).unsqueeze(1)  # Project candidate\n",
    "3. Compute attention scores: alpha_ = tanh(bmm(x_, M_)) * mask\n",
    "4. Compute softmax: alpha_ = softmax(alpha_, dim=2)\n",
    "5. Normalize over valid positions: alpha = alpha_masked / alpha_sum\n",
    "6. Compute attention pool: attn_pool = bmm(alpha, M.transpose(0, 1))\n",
    "\n",
    "# Output: attn_pool (attended vector), alpha (attention weights)\n",
    "```\n",
    "\n",
    "## Key Properties\n",
    "\n",
    "- The attention mechanism is \"matching\" style - it compares the candidate directly against memory entries.\n",
    "- Mask support allows for variable-length sequences.\n",
    "- The tanh activation provides non-linearity in attention computation.\n",
    "- Normalization ensures attention weights sum to 1 over valid positions.\n",
    "\n",
    "This implementation is commonly used in dialogue systems and other tasks where you need to match a current state against a memory of previous states.\n",
    "\n",
    "This explanation provides both the mathematical formulation and the connection to the implementation details in the code. You can adjust the level of detail or notation style to match your project's conventions.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
