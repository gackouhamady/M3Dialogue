{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intégration d'une Architecture Hybride GCN + Transformer dans DialogueGCN\n",
    "\n",
    "## Introduction\n",
    "Cette documentation décrit comment intégrer une architecture hybride combinant Graph Neural Networks (GCN) et Transformers dans DialogueGCN.\n",
    "\n",
    "## 1. Remplacement de RGCNConv par GATConv\n",
    "**Fichier**: `models.py`\n",
    "\n",
    "**Lignes concernées**: ~723-875 (classe `GraphNetwork`)\n",
    "\n",
    "### Modification du réseau de graphe\n",
    "```python\n",
    "from torch_geometric.nn import GATConv  # Remplacer l'import RGCNConv\n",
    "\n",
    "class GraphNetwork(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes, num_relations, max_seq_len, \n",
    "                 hidden_size=64, dropout=0.5, no_cuda=False, use_gat=True):\n",
    "        super(GraphNetwork, self).__init__()\n",
    "        \n",
    "        self.use_gat = use_gat\n",
    "        \n",
    "        if self.use_gat:\n",
    "            # Utilisation de Graph Attention Networks\n",
    "            self.conv1 = GATConv(num_features, hidden_size, heads=4, dropout=dropout)\n",
    "            self.conv2 = GATConv(hidden_size*4, hidden_size, heads=1, dropout=dropout)\n",
    "        else:\n",
    "            # Version originale avec RGCN\n",
    "            self.conv1 = RGCNConv(num_features, hidden_size, num_relations, num_bases=30)\n",
    "            self.conv2 = GraphConv(hidden_size, hidden_size)\n",
    "        \n",
    "        self.matchatt = MatchingAttention(num_features+hidden_size, num_features+hidden_size, att_type='general2')\n",
    "        self.linear = nn.Linear(num_features+hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.smax_fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.no_cuda = no_cuda\n",
    "\n",
    "    def forward(self, x, edge_index, edge_norm, edge_type, seq_lengths, umask, nodal_attn, avec):\n",
    "        if self.use_gat:\n",
    "            out = F.elu(self.conv1(x, edge_index))\n",
    "            out = F.dropout(out, p=self.dropout.p, training=self.training)\n",
    "            out = self.conv2(out, edge_index)\n",
    "        else:\n",
    "            out = self.conv1(x, edge_index, edge_type)\n",
    "            out = self.conv2(out, edge_index)\n",
    "            \n",
    "        emotions = torch.cat([x, out], dim=-1)\n",
    "        log_prob = classify_node_features(emotions, seq_lengths, umask, \n",
    "                                        self.matchatt, self.linear, \n",
    "                                        self.dropout, self.smax_fc, \n",
    "                                        nodal_attn, avec, self.no_cuda)\n",
    "        return log_prob\n",
    "```\n",
    "\n",
    "## 2. Ajout d'une Couche Transformer\n",
    "### Nouvelle classe `TransformerEncoderLayer`\n",
    "```python\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.activation = F.relu\n",
    "\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n",
    "                              key_padding_mask=src_key_padding_mask)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "```\n",
    "\n",
    "## 3. Modification de `DialogueGCNModel`\n",
    "**Lignes concernées**: ~878-1030 (classe `DialogueGCNModel`)\n",
    "\n",
    "```python\n",
    "class DialogueGCNModel(nn.Module):\n",
    "    def __init__(self, ..., use_transformer=False, transformer_layers=2):\n",
    "        ...\n",
    "        self.use_transformer = use_transformer\n",
    "        \n",
    "        if self.use_transformer:\n",
    "            self.transformer = nn.ModuleList([\n",
    "                TransformerEncoderLayer(2*D_e, nhead=4, \n",
    "                                      dim_feedforward=4*2*D_e, \n",
    "                                      dropout=dropout)\n",
    "                for _ in range(transformer_layers)\n",
    "            ])\n",
    "            self.pos_encoder = PositionalEncoding(2*D_e, dropout)\n",
    "        \n",
    "    def forward(self, U, qmask, umask, seq_lengths):\n",
    "        ...\n",
    "        if self.use_transformer:\n",
    "            emotions = emotions.permute(1, 0, 2)  # (batch, seq_len, dim)\n",
    "            src_key_padding_mask = torch.zeros(emotions.size(0), emotions.size(1)).bool()\n",
    "            for i, length in enumerate(seq_lengths):\n",
    "                src_key_padding_mask[i, length:] = True\n",
    "            if not self.no_cuda:\n",
    "                src_key_padding_mask = src_key_padding_mask.cuda()\n",
    "            emotions = self.pos_encoder(emotions)\n",
    "            for layer in self.transformer:\n",
    "                emotions = layer(emotions, src_key_padding_mask=src_key_padding_mask)\n",
    "            emotions = emotions.permute(1, 0, 2)\n",
    "        ...\n",
    "        return log_prob, edge_index, edge_norm, edge_type, edge_index_lengths\n",
    "```\n",
    "\n",
    "## 4. Ajout de `PositionalEncoding`\n",
    "```python\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "```\n",
    "\n",
    "## 5. Configuration Recommandée\n",
    "```python\n",
    "model = DialogueGCNModel(\n",
    "    base_model='GRU',\n",
    "    D_m=100, D_g=150, D_p=150, D_e=100, D_h=100, D_a=100,\n",
    "    graph_hidden_size=64,\n",
    "    n_speakers=2,\n",
    "    max_seq_len=100,\n",
    "    window_past=10,\n",
    "    window_future=10,\n",
    "    n_classes=7,\n",
    "    dropout_rec=0.5,\n",
    "    dropout=0.5,\n",
    "    use_transformer=True,\n",
    "    transformer_layers=2\n",
    ")\n",
    "```\n",
    "\n",
    "## Avantages de l'Architecture Hybride\n",
    "- **GAT (Graph Attention Networks)**:\n",
    "  - Meilleure modélisation des relations locales\n",
    "  - Attention dynamique sur les voisins dans le graphe\n",
    "\n",
    "- **Transformer**:\n",
    "  - Capture les dépendances longue portée\n",
    "  - Attention globale sur toute la conversation\n",
    "\n",
    "- **Combinaison des forces**:\n",
    "  - Le GAT gère les relations locales entre utterances\n",
    "  - Le Transformer capture les motifs globaux\n",
    "  - Meilleure performance sur les conversations longues"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
