# **Tache  1: Intégration des données**
## Références Officielles
### 1. MELD Dataset
- **Citation** :  
  S. Poria et al., "MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations", *ACL 2019*.  
  **Lien** : [https://aclanthology.org/P19-1050/](https://aclanthology.org/P19-1050/)

### 2. GLoVe Embeddings
- **Citation** :  
  J. Pennington, R. Socher, C. D. Manning, "GloVe: Global Vectors for Word Representation", *EMNLP 2014*.  
  **Lien** : [https://aclanthology.org/D14-1162/](https://aclanthology.org/D14-1162/)

### 3. BERT Model
- **Citation** :  
  J. Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", *NAACL 2019*.  
  **Lien** : [https://aclanthology.org/N19-1423/](https://aclanthology.org/N19-1423/)

### 4. Log-Mel Spectrograms (CNN Audio)
- **Citation** :  
  S. Hershey et al., "CNN Architectures for Large-Scale Audio Classification", *ICASSP 2017*.  
  **Lien** : [https://ieeexplore.ieee.org/document/7952261](https://ieeexplore.ieee.org/document/7952261)

### 5. MTCNN Face Detection
- **Citation** :  
  K. Zhang et al., "Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks", *IEEE Signal Processing Letters 2016*.  
  **Lien** : [https://ieeexplore.ieee.org/document/7553523](https://ieeexplore.ieee.org/document/7553523)

### 6. IEMOCAP Dataset
- **Citation** :  
  C. Busso et al., "IEMOCAP: Interactive emotional dyadic motion capture database", *Journal of Language Resources and Evaluation 2008*.  
  **Lien** : [https://sail.usc.edu/iemocap/](https://sail.usc.edu/iemocap/)