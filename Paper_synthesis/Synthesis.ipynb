{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2716f022",
   "metadata": {},
   "source": [
    "# Analyse de l‚Äôarticle : DialogueGCN ‚Äì A Graph Convolutional Neural Network for Emotion Recognition in Conversation\n",
    "\n",
    "## üìå Probl√©matique abord√©e\n",
    "\n",
    "La t√¢che de **reconnaissance des √©motions en conversation (ERC)** est essentielle pour d√©velopper des syst√®mes de dialogue empathiques et adapt√©s √† l'utilisateur. La plupart des approches pr√©c√©dentes utilisent des r√©seaux r√©currents (RNN, GRU, LSTM), mais ces derniers pr√©sentent des **limitations dans la mod√©lisation du contexte √† long terme** et ignorent souvent les **d√©pendances inter- et intra-locuteurs**.  \n",
    "**DialogueGCN** cherche √† surmonter ces limitations en mod√©lisant les interactions conversationnelles comme un **graphe dirig√©** o√π les **relations entre locuteurs** et les **positions relatives des √©nonc√©s** sont int√©gr√©es explicitement.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ M√©thodologie\n",
    "\n",
    "DialogueGCN repose sur une architecture en trois parties :\n",
    "\n",
    "1. **Encodage du contexte s√©quentiel** √† l‚Äôaide d‚Äôun GRU bidirectionnel.\n",
    "2. **Encodage du contexte au niveau du locuteur** via un r√©seau de neurones convolutif sur graphe (GCN).\n",
    "3. **Classification √©motionnelle** avec concat√©nation des repr√©sentations pr√©c√©dentes, suivie d'une attention contextuelle et d‚Äôun classifieur dense.\n",
    "\n",
    "Chaque √©nonc√© est trait√© comme un n≈ìud d‚Äôun graphe, avec des **ar√™tes √©tiquet√©es selon les d√©pendances temporelles et les relations entre locuteurs**.\n",
    "\n",
    "---\n",
    "\n",
    "## üìê Formulation math√©matique du mod√®le\n",
    "\n",
    "Soit une conversation contenant $$N$$ √©nonc√©s $$u_1, u_2, ..., u_N$$. Chaque √©nonc√© $$u_i$$ est √©mis par un locuteur $$p_{s(u_i)}$$, et est repr√©sent√© par un vecteur de caract√©ristiques $$u_i \\in \\mathbb{R}^{D_m}$$.\n",
    "\n",
    "**Encodage s√©quentiel :**\n",
    "$$\n",
    "g_i = \\overleftrightarrow{GRU}_S(g_{i(+,‚àí)1}, u_i)\n",
    "$$\n",
    "\n",
    "**Construction du graphe** :  \n",
    "Le graphe est d√©fini comme $$G = (V, E, R, W)$$ o√π :\n",
    "\n",
    "- $ V $ : ensemble des n≈ìuds (√©nonc√©s),\n",
    "- $ E $ : ensemble des ar√™tes dirig√©es,\n",
    "- $ R $ : types de relations (ex: $$p_1 \\rightarrow p_2$$, $$p_1 \\rightarrow p_1$$, etc.),\n",
    "- $ W $ : poids des ar√™tes d√©finis par attention.\n",
    "\n",
    "**Poids d‚Äôattention entre les n≈ìuds :**\n",
    "$$\n",
    "\\alpha_{ij} = \\text{softmax}(g_i^T W_e [g_{i-p}, ..., g_{i+f}])\n",
    "$$\n",
    "\n",
    "**Propagation dans le graphe :**\n",
    "1√®re couche GCN :\n",
    "$$\n",
    "h_i^{(1)} = \\sigma\\left( \\sum_{r \\in R} \\sum_{j \\in \\mathcal{N}_i^r} \\frac{\\alpha_{ij}}{c_{i,r}} W_r^{(1)} g_j + \\alpha_{ii} W_0^{(1)} g_i \\right)\n",
    "$$\n",
    "\n",
    "2e couche GCN :\n",
    "$$\n",
    "h_i^{(2)} = \\sigma\\left( \\sum_{j \\in \\mathcal{N}_i^r} W^{(2)} h_j^{(1)} + W_0^{(2)} h_i^{(1)} \\right)\n",
    "$$\n",
    "\n",
    "**Classification finale :**\n",
    "$$\n",
    "h_i = [g_i, h_i^{(2)}]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\beta_i = \\text{softmax}(h_i^T W_\\beta [h_1, ..., h_N])\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde{h}_i = \\beta_i [h_1, ..., h_N]^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_i = \\text{softmax}(W_{\\text{softmax}}(\\text{ReLU}(W_l \\tilde{h}_i + b_l)) + b_{\\text{softmax}})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## üßæ Jeux de donn√©es et m√©triques\n",
    "\n",
    "### Jeux de donn√©es utilis√©s :\n",
    "1. **IEMOCAP** : dialogues dyadiques annot√©s avec 6 √©motions.\n",
    "2. **AVEC** : dialogues homme-agent avec 4 dimensions √©motionnelles continues.\n",
    "3. **MELD** : dialogues multi-locuteurs issus de la s√©rie *Friends*, avec 7 √©motions.\n",
    "\n",
    "### M√©triques de performance :\n",
    "- **IEMOCAP & MELD** : $$F_1$$-score, exactitude (Accuracy).\n",
    "- **AVEC** : **MAE** (mean absolute error) sur chaque dimension continue.\n",
    "\n",
    "---\n",
    "\n",
    "## üåü Contribution originale\n",
    "\n",
    "1. Introduction d‚Äôun mod√®le **graphique convolutionnel (DialogueGCN)** pour mod√©liser explicitement les d√©pendances **inter- et intra-locuteurs**.\n",
    "2. Construction dynamique du graphe avec des relations bas√©es sur les **locuteurs** et les **positions temporelles**.\n",
    "3. Encodage du **contexte local** √† travers **deux couches de GCN**.\n",
    "4. Am√©lioration significative par rapport √† l‚Äô√©tat de l‚Äôart, notamment DialogueRNN, sur **tous les jeux de donn√©es**.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Points forts et limites\n",
    "\n",
    "### ‚úÖ Points forts :\n",
    "- Mod√©lisation explicite des **relations interpersonnelles** dans la conversation.\n",
    "- Bonne capacit√© √† capturer le **contexte long terme** via le graphe.\n",
    "- R√©sultats **sup√©rieurs √† l‚Äô√©tat de l‚Äôart** (ex: $$64.18\\%$$ F1 sur IEMOCAP).\n",
    "\n",
    "### ‚ùå Limites :\n",
    "- **Co√ªt computationnel √©lev√©** d√ª √† la construction du graphe complet.\n",
    "- D√©pendance √† des **fen√™tres de contexte pr√©-d√©finies**.\n",
    "- Moins performant dans des contextes √† **plusieurs locuteurs avec peu d‚Äô√©nonc√©s chacun** (ex: MELD).\n",
    "- Mod√®le **monomodale** (texte uniquement), sans exploitation des modalit√©s audio/vid√©o pourtant disponibles.\n",
    "\n",
    "---\n",
    "\n",
    "## üìà Perspectives d‚Äôam√©lioration\n",
    "\n",
    "- Int√©gration de **modalit√©s suppl√©mentaires** (audio, visuel).\n",
    "- R√©duction du **co√ªt de construction des graphes** via des m√©canismes d‚Äôattention ou de sparsit√©.\n",
    "- D√©veloppement d‚Äôun mod√®le **plus adaptatif** aux conversations multi-parties.\n",
    "- Exploration de **strat√©gies de pr√©-entra√Ænement** sur des dialogues ouverts.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaa99d8",
   "metadata": {},
   "source": [
    "# Synth√®se de l'article : *MMGCN: Multimodal Fusion via Deep Graph Convolution Network for Emotion Recognition in Conversation*\n",
    "\n",
    "## Probl√©matique abord√©e\n",
    "L'article s'attaque au probl√®me de la reconnaissance des √©motions dans les conversations (*Emotion Recognition in Conversation*, ERC), en contexte multimodal. La majorit√© des approches pr√©c√©dentes se focalisent sur la modalit√© textuelle seule ou fusionnent les modalit√©s (texte, audio, visuel) de mani√®re simple (par concat√©nation), sans consid√©rer finement les d√©pendances inter- et intra-modales, ni l'information structurelle comme l'identit√© du locuteur ou les d√©pendances √† longue distance dans les dialogues.\n",
    "\n",
    "## M√©thodologie propos√©e\n",
    "Les auteurs introduisent **MMGCN** (*Multimodal fused Graph Convolutional Network*), un mod√®le √† base de GCN en domaine spectral, profond, exploitant les trois modalit√©s (texte, audio, visuel) et l'identit√© des locuteurs. Il s'appuie sur une construction de graphe o√π chaque √©nonc√© est un triplet de n≈ìuds (un par modalit√©), connect√©s √† ceux des autres √©nonc√©s selon la similarit√©, la modalit√©, et le locuteur. Un encodeur de modalit√©s extrait les caract√©ristiques contextuelles de chaque modalit√© (BiLSTM pour le texte, MLP pour audio/visuel), avant passage dans le GCN. La sortie est ensuite classifi√©e en √©motions.\n",
    "\n",
    "## Formulation math√©matique du mod√®le\n",
    "\n",
    "Un dialogue est une suite d'√©nonc√©s :\n",
    "$$\n",
    "\\{u_1, u_2, ..., u_N\\} \\text{ o√π } u_i = \\{u_i^a, u_i^v, u_i^t\\}\n",
    "$$\n",
    "\n",
    "Encodage contextuel par modalit√© :\n",
    "$$\n",
    "\\begin{align*}\n",
    "h_i^t &= [\\overrightarrow{\\text{LSTM}}(u_i^t), \\overleftarrow{\\text{LSTM}}(u_i^t)] \\\\\n",
    "h_i^a &= W_e^a u_i^a + b_e^a \\\\\n",
    "h_i^v &= W_e^v u_i^v + b_e^v\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Ajout de l'embedding locuteur :\n",
    "$$\n",
    "S_i = W_s s_i + b_s\n",
    "$$\n",
    "\n",
    "Construction du graphe :\n",
    "- N≈ìuds : $$v_i^a = [h_i^a, S_i],\\ v_i^v = [h_i^v, S_i],\\ v_i^t = [h_i^t, S_i]$$\n",
    "- Ar√™tes intra-modales pond√©r√©es par :\n",
    "$$\n",
    "A_{ij} = 1 - \\frac{\\arccos(\\text{sim}(n_i, n_j))}{\\pi}\n",
    "$$\n",
    "- Ar√™tes inter-modales pond√©r√©es par :\n",
    "$$\n",
    "A_{ij} = \\gamma \\left(1 - \\frac{\\arccos(\\text{sim}(n_i, n_j))}{\\pi}\\right)\n",
    "$$\n",
    "\n",
    "Propagation GCN :\n",
    "$$\n",
    "H^{(l+1)} = \\sigma\\left(((1-\\alpha) \\tilde{P} H^{(l)} + \\alpha H^{(0)}) ((1-\\beta^{(l)}) I + \\beta^{(l)} W^{(l)})\\right)\n",
    "$$\n",
    "avec $$\\beta^{(l)} = \\log(\\eta l + 1)$$\n",
    "\n",
    "Classification :\n",
    "$$\n",
    "\\begin{align*}\n",
    "g_i &= [g_i^a, g_i^v, g_i^t] \\\\\n",
    "e_i &= [h'_i, g_i] \\\\\n",
    "l_i &= \\text{ReLU}(W_l e_i + b_l) \\\\\n",
    "P_i &= \\text{Softmax}(W_{smax} l_i + b_{smax}) \\\\\n",
    "\\hat{y}_i &= \\arg\\max_k P_i[k]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Fonction de perte :\n",
    "$$\n",
    "\\mathcal{L} = - \\frac{1}{\\sum_{s=1}^N c^{(s)}} \\sum_{i=1}^N \\sum_{j=1}^{c^{(i)}} \\log P_{i,j}[y_{i,j}] + \\lambda \\|\\theta\\|^2\n",
    "$$\n",
    "\n",
    "## Jeux de donn√©es et m√©triques\n",
    "\n",
    "- **IEMOCAP** : dialogues dyadiques (2 locuteurs), 151 dialogues, 7433 √©nonc√©s, 6 classes d'√©motions\n",
    "- **MELD** : dialogues multi-participants, 13708 √©nonc√©s, 7 √©motions\n",
    "- **M√©trique** : F1-score moyen pond√©r√©\n",
    "\n",
    "## Contribution originale\n",
    "- Introduction d'un **GCN spectral profond** multimodal avec r√©sidus adaptatifs\n",
    "- Fusion **fine** des modalit√©s via un graphe complet, connect√© intra/inter-modalit√© et enrichi par locuteur\n",
    "- Am√©lioration **significative** des performances sur deux benchmarks ERC\n",
    "\n",
    "## Forces et limites de la m√©thode\n",
    "\n",
    "### Forces\n",
    "- Exploite efficacement les **d√©pendances longues** dans le dialogue\n",
    "- Int√®gre **modalit√©s et locuteur** dans la structure du graphe\n",
    "- Approche **g√©n√©ralisable** et extensible (fonctionne avec + de modalit√©s / locuteurs)\n",
    "- Meilleure performance SOTA sur IEMOCAP et MELD\n",
    "\n",
    "### Limites\n",
    "- Co√ªteux en **ressources de calcul** (graphe complet, GCN profonds)\n",
    "- Besoin de **donn√©es align√©es** modalit√©/locuteur\n",
    "- Ne traite pas les cas d'information **manquante par modalit√©**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae0de86",
   "metadata": {},
   "source": [
    "# Synth√®se de l'article : *RBA-GCN: Relational Bilevel Aggregation Graph Convolutional Network for Emotion Recognition*\n",
    "\n",
    "## Probl√©matique abord√©e\n",
    "Cet article aborde la reconnaissance des √©motions dans les conversations (ERC), en mettant l'accent sur les limites des mod√®les GCN existants :\n",
    "- perte d'information discriminante due √† l'agr√©gation de n≈ìuds redondants,\n",
    "- incapacit√© √† capturer efficacement les d√©pendances contextuelles √† longue port√©e,\n",
    "- fusion faible des modalit√©s (texte, audio, visuel).\n",
    "\n",
    "## M√©thodologie propos√©e\n",
    "Les auteurs proposent **RBA-GCN**, un mod√®le compos√© de trois modules principaux :\n",
    "- **Graph Generation Module (GGM)** : construction d'un graphe o√π chaque √©nonc√© est repr√©sent√© par 3 n≈ìuds (texte, audio, visuel), connect√©s selon l'ordre de la conversation et l'appartenance modale.\n",
    "- **Similarity-based Cluster Building Module (SCBM)** : construction de clusters par similarit√© cosinus afin de filtrer les n≈ìuds non informatifs.\n",
    "- **Bilevel Aggregation Module (BiAM)** : agr√©gation hi√©rarchique (intra-cluster puis inter-cluster) des n≈ìuds similaires pour mettre √† jour les repr√©sentations.\n",
    "\n",
    "## Formulation math√©matique du mod√®le\n",
    "\n",
    "- Repr√©sentation des modalit√©s pour un √©nonc√© $$u_i$$ :\n",
    "$$\n",
    "u_i = \\{u_i^t, u_i^v, u_i^a\\}\n",
    "$$\n",
    "\n",
    "- Encodage contextuel par BiLSTM :\n",
    "$$\n",
    "g_i^x = [\\overrightarrow{\\text{LSTM}}(u_i^x), \\overleftarrow{\\text{LSTM}}(u_i^x)] \\text{ avec } x \\in \\{t, v, a\\}\n",
    "$$\n",
    "\n",
    "- Similarit√© cosinus entre le n≈ìud cible $$o$$ et un n≈ìud $$u$$ :\n",
    "$$\n",
    "s(u, o) = 1 - \\frac{\\arccos(\\text{sim}(f_u, f_o))}{\\pi}\n",
    "$$\n",
    "\n",
    "- D√©finition des clusters :\n",
    "$$\n",
    "\\tau(u, o) = \\lfloor \\gamma \\cdot s(u, o) \\rfloor, \\text{ si } u \\in C_g(o) \\text{ ou } (u \\in D_g(o) \\wedge s(u,o) \\geq \\rho)\n",
    "$$\n",
    "\n",
    "- Agr√©gation hi√©rarchique niveau 1 (dans un cluster $$r$$) :\n",
    "$$\n",
    "e_o(r) = \\frac{1}{|\\text{Clusters}(r)|} \\sum_{u \\in S_s(o)} \\delta(\\tau(u,o), r) \\cdot \\sigma^{(r)}(g_u)\n",
    "$$\n",
    "\n",
    "- Agr√©gation niveau 2 (target node + clusters) :\n",
    "$$\n",
    "h_i = \\sigma(W [e_o(r) \\| g_i])\n",
    "$$\n",
    "\n",
    "- Pr√©diction finale :\n",
    "$$\n",
    "\\begin{align*}\n",
    "l_i &= \\sigma(W_l h_i + b_l) \\\\\n",
    "p_i &= \\text{Softmax}(W_{smax} l_i + b_{smax}) \\\\\n",
    "\\hat{y}_i &= \\arg\\max(p_i)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- Fonction de perte :\n",
    "$$\n",
    "\\mathcal{L} = - \\frac{1}{\\sum_i N_i} \\sum_{i=1}^K \\sum_{j=1}^{N_i} \\sum_{m=1}^C y_{i,j}^{(m)} \\log(p_{i,j}^{(m)})\n",
    "$$\n",
    "\n",
    "## Jeux de donn√©es et m√©triques\n",
    "\n",
    "- **IEMOCAP** : 7433 √©nonc√©s, 6 √©motions : \\textit{happy, sad, neutral, angry, excited, frustrated}\n",
    "- **MELD** : 13708 √©nonc√©s, 7 √©motions : \\textit{anger, disgust, fear, joy, neutral, sadness, surprise}\n",
    "- **M√©trique** : F1-score moyen pond√©r√© (WAF1)\n",
    "\n",
    "## Contribution originale\n",
    "- Proposition d'un mod√®le **RBA-GCN** capturant les interactions multimodales et contextuelles via des clusters de similarit√©\n",
    "- Introduction de l'agr√©gation hi√©rarchique **BiAM** pr√©servant l'information discriminante\n",
    "- Am√©lioration de 2 √† 5 points de F1-score sur IEMOCAP et MELD\n",
    "\n",
    "## Forces et limites de la m√©thode\n",
    "\n",
    "### Forces\n",
    "- Captation **efficace** des d√©pendances contextuelles longues sans GCN multi-couches\n",
    "- **Filtrage intelligent** des n≈ìuds peu informatifs par mesure de similarit√©\n",
    "- Fusion multimodale plus **fine** et explicable que les concat√©nations\n",
    "- R√©sultats SOTA sur deux benchmarks majeurs\n",
    "\n",
    "### Limites\n",
    "- Sensibilit√© au choix des **hyperparam√®tres** $$\\gamma$$ et $$\\rho$$\n",
    "- Augmentation de la **complexit√© de calcul** par la construction de clusters et l'agr√©gation bi-niveau\n",
    "- Besoin de modalit√©s align√©es pour chaque √©nonc√©\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb15f85",
   "metadata": {},
   "source": [
    "# Synth√®se de l'article : *Dynamic Graph Neural Ordinary Differential Equation Network for Multi-modal Emotion Recognition in Conversation*\n",
    "\n",
    "## Probl√©matique abord√©e\n",
    "Cet article aborde la reconnaissance des √©motions dans les conversations multimodales (texte, audio, vid√©o). Les m√©thodes existantes bas√©es sur les GCN souffrent de deux limitations majeures :\n",
    "- surajustement et perte de g√©n√©ralisation des GCN profonds (over-smoothing)\n",
    "- incapacit√© √† mod√©liser les **d√©pendances temporelles continues** des √©motions d'un locuteur\n",
    "\n",
    "## M√©thodologie propos√©e\n",
    "Les auteurs proposent le mod√®le **DGODE** (Dynamic Graph Neural Ordinary Differential Equation), combinant :\n",
    "- un **graphe adaptatif MixHop** pour capturer les relations d'ordre sup√©rieur entre n≈ìuds\n",
    "- une mod√©lisation dynamique par **√©quation diff√©rentielle ordinaire (ODE)** sur graphes, afin de suivre l'√©volution continue des √©motions dans le temps\n",
    "\n",
    "Le pipeline inclut :\n",
    "1. Extraction des caract√©ristiques multimodales via RoBERTa, DenseNet et openSMILE\n",
    "2. Int√©gration de l'identit√© du locuteur dans les embeddings\n",
    "3. Agr√©gation adaptative multi-hop\n",
    "4. Mod√©lisation temporelle avec ODE sur graphe\n",
    "\n",
    "## Formulation math√©matique du mod√®le\n",
    "\n",
    "- Mod√©lisation d'une conversation comme :\n",
    "$$\n",
    "C = [(u_1, s_1), (u_2, s_2), ..., (u_M, s_M)]\n",
    "$$\n",
    "\n",
    "- Int√©gration du locuteur dans les embeddings :\n",
    "$$\n",
    "P_i = W_p p_i \\quad ; \\quad h_i^m = c_i^m + S_i, \\text{ avec } m \\in \\{t, a, v\\}\n",
    "$$\n",
    "\n",
    "- Agr√©gation adaptive MixHop (discr√®te) :\n",
    "$$\n",
    "H^{(n+1)} = \\sum_{n=1}^N \\hat{A}^n H^{(n)} W + H^{(0)}\n",
    "$$\n",
    "\n",
    "- Mod√©lisation ODE (continue) :\n",
    "$$\n",
    "\\frac{dH(t)}{dt} = \\frac{1}{N} \\sum_{n=1}^N \\left( \\ln(\\hat{A}) H(t) + H(t) \\ln(W) + E \\right)\n",
    "$$\n",
    "\n",
    "avec :\n",
    "$$\n",
    "H(t) = ODESolver\\left( \\frac{dH(t)}{dt}, H_0, t \\right)\n",
    "$$\n",
    "\n",
    "- Pr√©diction d'√©motion :\n",
    "$$\n",
    "\\begin{align*}\n",
    "l_i &= \\text{ReLU}(W_l H_i + b_l) \\\\\n",
    "p_i &= \\text{Softmax}(W_{smax} l_i + b_{smax}) \\\\\n",
    "\\hat{y}_i &= \\arg\\max_j(p_{ij})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "## Jeux de donn√©es et m√©triques\n",
    "- **IEMOCAP** : 7433 √©nonc√©s, 6 √©motions : happy, sad, neutral, angry, excited, frustrated\n",
    "- **MELD** : 13708 √©nonc√©s, 7 √©motions : anger, disgust, fear, joy, neutral, sadness, surprise\n",
    "- **M√©triques** : F1-score moyen pond√©r√© (W-F1) et F1 standard\n",
    "\n",
    "## Contribution originale\n",
    "- Introduction de **DGODE**, premier mod√®le MERC √† ODE sur graphe\n",
    "- Int√©gration d'un **m√©canisme MixHop adaptatif** pour explorer des voisins √† plusieurs sauts\n",
    "- R√©duction du surajustement (overfitting) et sur-lissage (over-smoothing) dans les GCN profonds\n",
    "- **Stabilit√© des performances** avec le nombre de couches GCN croissant\n",
    "\n",
    "## Forces et limites de la m√©thode\n",
    "\n",
    "### Forces\n",
    "- Capture fine des **d√©pendances temporelles continues** entre les √©motions\n",
    "- Am√©lioration SOTA sur IEMOCAP et MELD\n",
    "- Utilisation judicieuse des ODEs pour l'√©volution dynamique des n≈ìuds\n",
    "- √âtudes d'ablation compl√®tes montrant l'apport de chaque composant\n",
    "\n",
    "### Limites\n",
    "- **Complexit√© num√©rique** due √† la r√©solution d'ODEs\n",
    "- √âtiquetage global des √©motions parfois incompatible avec une dynamique fine\n",
    "- Moins performant sur les √©motions **minoritaires ou similaires** (confusions entre \"happy\" et \"excited\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d3e7e8",
   "metadata": {},
   "source": [
    "# Synth√®se de l'article : *MultiEMO: An Attention-Based Correlation-Aware Multimodal Fusion Framework for Emotion Recognition in Conversations*\n",
    "\n",
    "## Probl√©matique abord√©e\n",
    "L'article traite de la reconnaissance des √©motions en conversation multimodale (ERC) en mettant en √©vidence trois probl√®mes principaux :\n",
    "1. La **fusion multimodale insuffisante** : les m√©thodes existantes utilisent souvent la concat√©nation simple des modalit√©s (texte, audio, visuel), sans mod√©liser leurs corr√©lations complexes.\n",
    "2. La **mauvaise performance sur les classes minoritaires** : les classes d'√©motions sont tr√®s d√©s√©quilibr√©es dans les benchmarks (notamment MELD).\n",
    "3. La **difficult√© √† distinguer les √©motions s√©mantiquement proches** (ex : anger vs. disgust).\n",
    "\n",
    "## M√©thodologie propos√©e\n",
    "Les auteurs introduisent **MultiEMO**, un cadre de fusion multimodale sensible aux corr√©lations, compos√© de quatre modules principaux :\n",
    "1. **VisExtNet** : un extracteur visuel bas√© sur MTCNN + ResNet-101 (pr√©entrain√© sur VGGFace2) pour extraire les expressions faciales.\n",
    "2. **DialogueRNN** : pour la mod√©lisation contextuelle dans les modalit√©s audio et visuelle.\n",
    "3. **MultiAttn** : module de fusion multimodale √† base de multi-head cross-attention bidirectionnel.\n",
    "4. **SWFC Loss** (Sample-Weighted Focal Contrastive Loss) : une fonction de perte adapt√©e aux classes d√©s√©quilibr√©es et s√©mantiquement proches.\n",
    "\n",
    "## Formulation math√©matique\n",
    "\n",
    "Chaque √©nonc√© $$u_i$$ est repr√©sent√© par :\n",
    "$$\n",
    "u_i = \\{u_i^t, u_i^a, u_i^v\\}\n",
    "$$\n",
    "\n",
    "**Fusion multimodale via MultiAttn (simplifi√©)** :\n",
    "1. Attention croissante texte ‚Üî audio :\n",
    "$$\n",
    "Q^{ta}_h = F_t^{(j-1)} W_Q^{ta}, \\quad K^{ta}_h = C^a W_K^{ta}, \\quad V^{ta}_h = C^a W_V^{ta}\n",
    "$$\n",
    "2. Sortie :\n",
    "$$\n",
    "A_h^{ta} = \\text{Softmax}\\left(\\frac{Q_h^{ta} K_h^{taT}}{\\sqrt{d}}\\right) V_h^{ta}, \\quad M^{ta} = \\text{Concat}(A_1^{ta}, ..., A_H^{ta}) W_O^{ta}\n",
    "$$\n",
    "3. Attention texte-audio + visuel, r√©p√©t√© sur $$T$$ couches :\n",
    "$$\n",
    "F_t^{(j)} = \\text{LayerNorm}(F_{t,av}^{(j)} + FFN(F_{t,av}^{(j)}))\n",
    "$$\n",
    "\n",
    "**Fusion finale et classification** :\n",
    "$$\n",
    "\\begin{align*}\n",
    "f_i &= f_i^t \\oplus f_i^a \\oplus f_i^v \\\\\n",
    "z_i &= W_z f_i + b_z \\\\\n",
    "l_i &= \\text{ReLU}(W_l z_i + b_l) \\\\\n",
    "p_i &= \\text{Softmax}(W_{\\text{smax}} l_i + b_{\\text{smax}}) \\\\\n",
    "\\hat{y}_i &= \\arg\\max_t p_i[t]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "**Fonction de perte** (combin√©e) :\n",
    "$$\n",
    "\\mathcal{L}_{\\text{train}} = \\frac{1}{N}(\\mu_1 \\mathcal{L}_{\\text{SWFC}} + \\mu_2 \\mathcal{L}_{\\text{Soft-HGR}} + (1 - \\mu_1 - \\mu_2) \\mathcal{L}_{CE}) + \\lambda \\|\\theta\\|_2^2\n",
    "$$\n",
    "\n",
    "### SWFC Loss (Sample-Weighted Focal Contrastive)\n",
    "$$\n",
    "\\mathcal{L}_{\\text{SWFC}} = - \\sum_{i=1}^M \\sum_{j=1}^{C^{(i)}} \\left(\\frac{N}{n_{y_{ij}}}\\right)^\\alpha \\cdot \\frac{1}{|R_{ij}|} \\sum_{z_g \\in R_{ij}} (1 - s_{jg})^\\gamma \\log s_{jg}\n",
    "$$\n",
    "avec :\n",
    "$$\n",
    "s_{jg} = \\frac{\\exp(z_{ij}^T z_g / \\tau)}{\\sum_{z_s \\in A_{ij}} \\exp(z_{ij}^T z_s / \\tau)}\n",
    "$$\n",
    "\n",
    "### Soft-HGR Loss (maximisation de la corr√©lation inter-modale)\n",
    "$$\n",
    "\\mathcal{L}_{\\text{Soft-HGR}} = - \\sum_{Q \\neq V \\in \\mathcal{F}} \\left( \\mathbb{E}[Q^T V] - \\frac{1}{2} \\text{Tr}(\\text{cov}(Q) \\text{cov}(V)) \\right)\n",
    "$$\n",
    "\n",
    "## Jeux de donn√©es et m√©triques\n",
    "- **IEMOCAP** : 7433 √©nonc√©s, 6 √©motions : happy, sad, neutral, angry, excited, frustrated\n",
    "- **MELD** : 13708 √©nonc√©s, 7 √©motions : anger, disgust, fear, joy, neutral, sadness, surprise\n",
    "- **M√©trique** : Weighted-F1 (F1 pond√©r√©)\n",
    "\n",
    "## Contribution originale\n",
    "- Proposition d‚Äôun **extracteur visuel VisExtNet** qui exclut les informations de sc√®ne non informatives\n",
    "- Fusion **fine et bidirectionnelle** des modalit√©s via **MultiAttn** (cross-attention)\n",
    "- Nouvelle fonction de perte **SWFC** pour g√©rer le d√©s√©quilibre et la proximit√© s√©mantique\n",
    "- **SOTA** sur MELD et IEMOCAP, en particulier sur les classes rares et ambigu√´s\n",
    "\n",
    "## Forces et limites\n",
    "\n",
    "### Forces\n",
    "- Fusion multimodale avanc√©e avec cross-attention bidirectionnel\n",
    "- Traitement efficace des √©motions difficiles √† distinguer (ex : anger vs. disgust)\n",
    "- Visualisation et analyse qualitatives approfondies (cf. heatmaps)\n",
    "- Fonction de perte adaptative √©labor√©e\n",
    "\n",
    "### Limites\n",
    "- **VisExtNet** ne distingue pas entre locuteurs et figurants ‚Üí risque de bruit visuel\n",
    "- **SWFC** n√©cessite des batchs tr√®s larges pour garantir des paires positives ‚Üí co√ªteux\n",
    "- Effets des hyperparam√®tres $$\\alpha, \\gamma, \\tau$$ peu explor√©s\n",
    "- Am√©liorations encore modestes pour les classes tr√®s rares\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c40a22",
   "metadata": {},
   "source": [
    "# Synth√®se de l'article : *TelME ‚Äì Teacher-leading Multimodal Fusion Network for Emotion Recognition in Conversation*\n",
    "\n",
    "## Probl√©matique abord√©e\n",
    "\n",
    "La reconnaissance des √©motions en conversation (ERC) multimodale repose sur l'exploitation combin√©e des modalit√©s textuelles, visuelles et audio. Toutefois :\n",
    "- la **modalit√© texte domine** largement les performances,\n",
    "- les modalit√©s **non-verbales (audio/visuel)** sont souvent faibles et peu informatives,\n",
    "- les approches actuelles traitent chaque modalit√© **de fa√ßon homog√®ne**, sans consid√©ration pour leurs contributions in√©gales,\n",
    "- la **fusion multimodale reste basique** (souvent concat√©nation), et ignore les sp√©cificit√©s entre modalit√©s.\n",
    "\n",
    "## M√©thodologie propos√©e : TelME\n",
    "\n",
    "TelME est un mod√®le de fusion multimodale dirig√© par l'information de la modalit√© texte (forte) et destin√© √† :\n",
    "- renforcer les modalit√©s audio et visuelle par **distillation de connaissances crois√©es** (cross-modal KD),\n",
    "- am√©liorer la fusion par un **shifting fusion attentionnel** (ASF) : les modalit√©s faibles corrigent le professeur texte.\n",
    "\n",
    "Il se compose de 3 modules :\n",
    "1. **Encodage multimodal** (Roberta, Data2Vec, TimeSformer)\n",
    "2. **Distillation crois√©e** (response-based + feature-based)\n",
    "3. **Attention-based Shifting Fusion** (recalibrage dynamique)\n",
    "\n",
    "## Formulation math√©matique du mod√®le\n",
    "\n",
    "### Encodage texte avec prompt\n",
    "$$\n",
    "\\begin{align*}\n",
    "C_k &= [\\langle s_i \\rangle, t_1, \\langle s_j \\rangle, t_2, ..., \\langle s_i \\rangle, t_k] \\\\\n",
    "P_k &= \\text{\"Now }\\langle s_i \\rangle\\text{ feels }\\langle \\text{mask} \\rangle\" \\\\\n",
    "F_k^T &= \\text{TextEncoder}(C_k\\langle /s \\rangle P_k)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### Encodage audio et visuel\n",
    "$$\n",
    "F_k^A = \\text{AudioEncoder}(a_k), \\quad F_k^V = \\text{VisualEncoder}(v_k)\n",
    "$$\n",
    "\n",
    "### Distillation crois√©e : pertes combin√©es\n",
    "$$\n",
    "\\mathcal{L}_{\\text{student}} = \\mathcal{L}_{\\text{cls}} + \\alpha \\mathcal{L}_{\\text{response}} + \\beta \\mathcal{L}_{\\text{feature}}\n",
    "$$\n",
    "\n",
    "#### Pertes de distillation par corr√©lation\n",
    "$$\n",
    "\\mathcal{L}_{\\text{inter}} = \\frac{\\tau^2}{B} \\sum_{i=1}^B \\left(1 - \\rho(Y_i^s, Y_i^t)\\right), \\quad \\mathcal{L}_{\\text{intra}} = \\frac{\\tau^2}{C} \\sum_{j=1}^C \\left(1 - \\rho(Y_j^s, Y_j^t)\\right)\n",
    "$$\n",
    "$$\n",
    "\\mathcal{L}_{\\text{response}} = \\mathcal{L}_{\\text{inter}} + \\mathcal{L}_{\\text{intra}}\n",
    "$$\n",
    "\n",
    "#### Distillation par similarit√©s inter-batch\n",
    "$$\n",
    "\\mathcal{L}_{\\text{feature}} = \\frac{1}{B} \\sum_{i=1}^B KL(P_i \\parallel Q_i)\n",
    "$$\n",
    "avec : $$P_i = \\text{softmax}(\\frac{M_{ij}}{\\tau}),\\ Q_i = \\text{softmax}(\\frac{M'_{ij}}{\\tau})$$\n",
    "\n",
    "### Attention-based Shifting Fusion\n",
    "$$\n",
    "\\begin{align*}\n",
    "F^{\\text{att}} &= \\text{SelfAttn}(F_k^A \\oplus F_k^V) \\\\\n",
    "g_k &= R(W_1 \\cdot [F_k^T, F^{\\text{att}}] + b_1) \\\\\n",
    "H_k &= g_k \\cdot (W_2 \\cdot F^{\\text{att}} + b_2) \\\\\n",
    "\\lambda &= \\min\\left(\\frac{\\|F_k^T\\|_2}{\\|H_k\\|_2} \\cdot \\theta, 1\\right) \\\\\n",
    "Z_k &= F_k^T + \\lambda \\cdot H_k\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### Classification finale\n",
    "$$\n",
    "\\hat{y}_k = \\arg\\max \\text{Softmax}(W Z_k + b)\n",
    "$$\n",
    "\n",
    "## Jeux de donn√©es et m√©triques\n",
    "- **MELD** : 13708 √©nonc√©s, 7 √©motions (conversation multi-locuteurs)\n",
    "- **IEMOCAP** : 7433 √©nonc√©s, 6 √©motions (dyades)\n",
    "- **M√©trique** : Weighted F1-score\n",
    "\n",
    "## Contribution originale\n",
    "- Premier mod√®le ERC √† utiliser **distillation croissante croisement modalit√©** pour renforcer les signaux faibles\n",
    "- Fusion √©motionnelle via **vecteurs de d√©placement** (shifting fusion)\n",
    "- D√©monstration exp√©rimentale : SOTA sur MELD, tr√®s fort sur IEMOCAP\n",
    "- Am√©lioration de la classification des √©motions minoritaires et ambigu√´s (e.g., fear, disgust)\n",
    "\n",
    "## Forces et limites de la m√©thode\n",
    "\n",
    "### Forces\n",
    "- Distillation efficace (texte ‚Üí audio/visuel) sans KL mais via corr√©lations\n",
    "- Fusion dynamique √©motionnelle : les signaux faibles influencent les forts\n",
    "- Am√©lioration nette sur les classes difficiles (fear, disgust)\n",
    "- Analyse pouss√©e : ablation, mod√®les mono-modaux, confusion matrices\n",
    "\n",
    "### Limites\n",
    "- Moindre qualit√© de la modalit√© **visuelle** (bruit, courtes dur√©es)\n",
    "- Performances **limite√©es par le d√©s√©quilibre des classes** (surtout sur MELD)\n",
    "- Sensibilit√© aux hyperparam√®tres de temp√©rature et de distillation\n",
    "\n",
    "TelME ouvre une voie prometteuse pour la fusion multimodale guid√©e par un enseignant fort, tout en permettant aux modalit√©s faibles d'enrichir la repr√©sentation finale de mani√®re adaptative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f26e3b",
   "metadata": {},
   "source": [
    "# Synth√®se de l'article : *Bi-stream Graph Learning based Multimodal Fusion for Emotion Recognition in Conversation (BiGMF)*\n",
    "\n",
    "## Probl√©matique abord√©e\n",
    "\n",
    "La reconnaissance des √©motions en conversation (ERC) multimodale est difficile √† cause :\n",
    "- du **conflit entre modalit√©s** lors de la fusion (texte, audio, visuel),\n",
    "- de **l'h√©t√©rog√©n√©it√© des donn√©es** multimodales,\n",
    "- de la fusion √©gale de modalit√©s de qualit√© variable (visuel vs texte),\n",
    "- et de la difficult√© √† capter √† la fois les **d√©pendances contextuelles intra-modales** et les **interactions inter-modales**.\n",
    "\n",
    "## M√©thodologie propos√©e : BiGMF\n",
    "\n",
    "BiGMF est une architecture en **double flux de graphes** combinant :\n",
    "- un **Unimodal Stream Graph Learning (UMGAT)** : pour capturer les d√©pendances longues *intra-modales* (par modalit√©)\n",
    "- un **Cross-modal Stream Graph Learning (CMGAT)** : pour mod√©liser les interactions explicites entre modalit√©s\n",
    "- une **perte de consistance inter-modale** pour forcer la compatibilit√© entre repr√©sentations crois√©es\n",
    "- un **module r√©siduel adaptatif** pour contrer l'over-smoothing des GNNs\n",
    "\n",
    "## Formulation math√©matique\n",
    "\n",
    "### Encodage des modalit√©s\n",
    "$$\n",
    "\\begin{align*}\n",
    "X^a &= \\text{FC}(\\text{Norm}(U^a)) \\\\\n",
    "X^v &= \\text{FC}(\\text{Norm}(U^v)) \\\\\n",
    "X^t &= \\overleftrightarrow{\\text{LSTM}}(\\text{Norm}(U^t))\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### Ajout des embeddings locuteur\n",
    "$$\n",
    "X_i^s = X_i + \\eta_i S \\quad \\text{o√π } i \\in \\{a, t, v\\}\n",
    "$$\n",
    "\n",
    "### Construction des graphes unimodaux (UMGAT)\n",
    "- Graphe complet $$G_i = (V_i, E_i, W_i)$$ par modalit√©\n",
    "- Poids des ar√™tes par self-attention :\n",
    "$$\n",
    "\\alpha^i = \\text{Softmax}\\left(\\frac{Q^i K^{iT}}{\\sqrt{d_k}}\\right)\n",
    "$$\n",
    "\n",
    "### Agr√©gation et mise √† jour de n≈ìud\n",
    "$$\n",
    "\\text{Neibor}(x_j^i) = \\big\\|_{h=1}^H \\sum_{k \\in V_j^i} \\alpha_{jk}^{i,h} W_v^{i,h} x_k^i\n",
    "$$\n",
    "$$\n",
    "x_j'^i = W_2^i \\cdot \\sigma\\left(W_1^i \\left[(x_j^i + \\text{Neibor}(x_j^i)) \\| (x_j^i \\odot \\text{Neibor}(x_j^i))\\right]\\right)\n",
    "$$\n",
    "\n",
    "### Module r√©siduel adaptatif\n",
    "$$\n",
    "X_u^i = X_{s,L}^i + \\text{Drop}(\\text{Linear}(X^i))\n",
    "$$\n",
    "\n",
    "### Construction des graphes crois√©s (CMGAT)\n",
    "- Noeuds : $$V_{i \\cup j}$$ avec $$i \\neq j$$\n",
    "- Ar√™tes bidirectionnelles entre modalit√©s\n",
    "- Poids par co-attention :\n",
    "$$\n",
    "\\alpha^{i|j} = \\text{Softmax}\\left(\\frac{Q^i K^{jT}}{\\sqrt{d_k}}\\right)\n",
    "$$\n",
    "\n",
    "### Agr√©gation crois√©e sparse (top-K)\n",
    "$$\n",
    "\\text{Neibor}(x_i^{i \\cup j}) = \\big\\|_{h=1}^H \\sum_{k \\in N_K(i)} \\alpha_{ik}^{i|j,h} W_v^{j|i,h} x_k^j\n",
    "$$\n",
    "$$\n",
    "x_i' = W_2 \\cdot \\sigma\\left(W_1[(x_i + \\text{Neibor}(x_i)) \\| (x_i \\odot \\text{Neibor}(x_i))]\\right)\n",
    "$$\n",
    "\n",
    "### Fusion finale\n",
    "$$\n",
    "X = (X_u^a + X_c^a) \\| (X_u^t + X_c^t) \\| (X_u^v + X_c^v)\n",
    "$$\n",
    "\n",
    "### Pr√©diction\n",
    "$$\n",
    "p_i = \\text{Softmax}(f_c(x_i)) \\quad ; \\quad \\hat{y}_i = \\arg\\max p_i\n",
    "$$\n",
    "\n",
    "### Pertes totales\n",
    "$$\n",
    "\\mathcal{L} = \\mathcal{L}_{\\text{avt}} + \\zeta_a \\mathcal{L}_a^{ce} + \\zeta_t \\mathcal{L}_t^{ce} + \\zeta_v \\mathcal{L}_v^{ce} + \\gamma \\mathcal{L}_{cl}\n",
    "$$\n",
    "avec perte de consistance :\n",
    "$$\n",
    "\\mathcal{L}_{cl} = \\|X_c^a - X_c^t\\|_2^2 + \\|X_c^a - X_c^v\\|_2^2 + \\|X_c^t - X_c^v\\|_2^2\n",
    "$$\n",
    "\n",
    "## Jeux de donn√©es et m√©triques\n",
    "- **IEMOCAP** : 7433 √©nonc√©s, 6 √©motions\n",
    "- **MELD** : 13708 √©nonc√©s, 7 √©motions\n",
    "- **M√©trique** : Weighted-F1 (wa-F1) et Accuracy\n",
    "\n",
    "## Contribution originale\n",
    "- Double architecture GNN : **UMGATs** pour le contexte intra-modal et **CMGATs** pour l'interaction inter-modal\n",
    "- Utilisation de **graphes h√©t√©rog√®nes** explicites pour r√©duire les conflits de fusion\n",
    "- **Co-attention sparse** pour s√©lectionner les liens utiles\n",
    "- Module r√©siduel adaptatif + perte de consistance multimodale\n",
    "- Sup√©riorit√© SOTA ou comparable sur IEMOCAP et MELD\n",
    "\n",
    "## Forces et limites\n",
    "\n",
    "### Forces\n",
    "- Captation explicite des **d√©pendances inter-/intra-modalit√©**\n",
    "- Architecture **modulaire et interpr√©table**\n",
    "- Bonne g√©rance de l'h√©t√©rog√©n√©it√© via bi-graphes\n",
    "- Performances solides, m√™me sur des dialogues courts (MELD)\n",
    "\n",
    "### Limites\n",
    "- CMGATs sensibles aux **bruits et redondances** (cross-modal)\n",
    "- Complexit√© accrue pour graphes + attention multi-t√™tes\n",
    "- Plus **difficile √† parall√©liser** que les mod√®les Transformer purs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ec9fe0",
   "metadata": {},
   "source": [
    "# Synth√®se de l'article : *Multimodal Emotion Recognition in Conversation Based on Hypergraphs (MER-HGraph)*\n",
    "\n",
    "## Probl√©matique abord√©e\n",
    "\n",
    "Les m√©thodes classiques d'ERC multimodal (Emotion Recognition in Conversation) s'appuient sur des mod√®les s√©quentiels (ex : RNN) ou des GCNs, qui :\n",
    "- n'exploitent pas les **interactions d'ordre sup√©rieur** entre modalit√©s,\n",
    "- perdent des informations contextuelles riches,\n",
    "- propagent uniquement des relations **binaires**, inad√©quates pour les donn√©es multimodales.\n",
    "\n",
    "## M√©thodologie propos√©e : MER-HGraph\n",
    "\n",
    "MER-HGraph introduit une architecture fond√©e sur les **hypergraphes** pour mod√©liser :\n",
    "- les **d√©pendances intra-modales** via des hypergraphes intra (Intra-HGraph),\n",
    "- les **interactions inter-modales** via des hypergraphes inter (Inter-HGraph),\n",
    "- et un **module de fen√™trage temporel dynamique** (DTWB) pour extraire localement et globalement des signaux acoustiques fiables.\n",
    "\n",
    "Le pipeline comprend :\n",
    "1. Extraction des features : DenseNet (visuel), OpenSMILE (audio), TextCNN (texte).\n",
    "2. Encodage des signaux audio par DTWB avec attention + Transformer\n",
    "3. Int√©gration des embeddings de locuteur (one-hot)\n",
    "4. Construction des hypergraphes (contextes pass√©s/futurs et modalit√©s crois√©es)\n",
    "5. Convolution sur hypergraphe\n",
    "6. Pr√©diction via couche dense + softmax\n",
    "\n",
    "## Formulation math√©matique\n",
    "\n",
    "### Encodage mono-modal\n",
    "$$\n",
    "\\begin{align*}\n",
    "x_i^A &= u_i^{A_2} \\oplus u_i^{A_3} \\\\\n",
    "x_i^T &= \\overleftrightarrow{\\text{LSTM}}(u_i^T) \\\\\n",
    "x_i^V &= W_e^V u_i^V + b_i^V\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### Encodage locuteur\n",
    "$$\n",
    "S_i = W_s s_i + b_s\n",
    "$$\n",
    "\n",
    "### Construction de l'hypergraphe\n",
    "- Intra-HGraph : connexions dans chaque modalit√© entre $$u_{i-P}, ..., u_i, ..., u_{i+F}$$\n",
    "- Inter-HGraph : connexions entre $$u_i^A, u_i^T, u_i^V$$ (m√™me √©nonc√©, modalit√©s diff√©rentes)\n",
    "\n",
    "### Matrices :\n",
    "- H : matrice d'incidence $$H \\in \\mathbb{R}^{N \\times M}$$ entre noeuds et hyperar√™tes\n",
    "- D : degr√©s de sommets, B : degr√©s d'ar√™tes\n",
    "\n",
    "### Convolution sur hypergraphe\n",
    "$$\n",
    "X^{(l+1)} = D^{-1} H W B^{-1} H^T X^{(l)}\n",
    "$$\n",
    "\n",
    "### Pr√©diction d'√©motion\n",
    "$$\n",
    "\\begin{align*}\n",
    "z_i &= \\text{ReLU}(W h_i + b) \\\\\n",
    "p_i &= \\text{Softmax}(W' z_i + b') \\\\\n",
    "\\hat{y}_i &= \\arg\\max_k p_i[k]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### Fonction de perte\n",
    "$$\n",
    "\\mathcal{L} = - \\sum_{i,j} y_{ij} \\log p_{ij} + \\lambda \\|W_{ls}\\|^2\n",
    "$$\n",
    "\n",
    "## Jeux de donn√©es et m√©triques\n",
    "- **IEMOCAP** : 7433 √©nonc√©s (6 √©motions)\n",
    "- **MELD** : 13708 √©nonc√©s (7 √©motions)\n",
    "- **M√©triques** : Accuracy, Weighted-F1\n",
    "\n",
    "## Contribution originale\n",
    "- Introduction de **hypergraphes intra- et inter-modaux** pour ERC\n",
    "- Mod√©lisation √† haut niveau des d√©pendances dans les dialogues\n",
    "- **DTWB** : fen√™trage dynamique attentionnel sur les signaux acoustiques\n",
    "- SOTA sur IEMOCAP (Acc 70.81%, wa-F1 70.37%) et MELD (Acc 62.76%, wa-F1 59.13%)\n",
    "\n",
    "## Forces et limites\n",
    "\n",
    "### Forces\n",
    "- Captation des **relations d'ordre sup√©rieur** entre √©nonc√©s et modalit√©s\n",
    "- Robustesse accrue aux bruits audio gr√¢ce au DTWB\n",
    "- Interpr√©tabilit√© des structures hypergraphiques\n",
    "- Performances SOTA ou tr√®s proches\n",
    "\n",
    "### Limites\n",
    "- Complexit√© du traitement hypergraphique (scalabilit√©, temps)\n",
    "- Pas de m√©canisme explicite d'attention dans la convolution\n",
    "- N√©cessite un bon tuning des fen√™tres contextuelles $$P, F$$\n",
    "\n",
    "MER-HGraph pose les bases d'une ERC hypergraphique multimodale fine, combinant dynamique temporelle, fusion explicite et haut-niveau d'interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046370dc",
   "metadata": {},
   "source": [
    "# Synth√®se de l'article : *Enhancing Emotion Recognition in Conversation through Emotional Cross-Modal Fusion and Inter-class Contrastive Learning*\n",
    "\n",
    "## Probl√©matique abord√©e\n",
    "\n",
    "Les mod√®les ERC multimodaux classiques pr√©sentent deux faiblesses majeures :\n",
    "1. Ils utilisent une fusion **na√Øve** entre modalit√©s (ex. concat√©nation), sans distinguer les apports sp√©cifiques (texte = contenu, audio = prosodie).\n",
    "2. Ils ne traitent pas le **d√©s√©quilibre entre les classes d'√©motions**, ce qui nuit √† la d√©tection des √©motions rares.\n",
    "\n",
    "## M√©thodologie propos√©e\n",
    "\n",
    "Les auteurs proposent un mod√®le en deux √©tapes :\n",
    "- **Joint-based Fusion Module (JFM)** : permet une fusion audio-texte par des vecteurs joints appris, tout en maintenant l'ind√©pendance de chaque modalit√©.\n",
    "- **Inter-class Contrastive Learning (ICL)** : optimise la s√©paration entre classes d'√©motions en rapprochant les √©chantillons de m√™me classe et en √©loignant les autres.\n",
    "\n",
    "Le pipeline repose sur RoBERTa (texte), STFT + Transformer (audio), et un apprentissage contrastif supervis√© par les √©tiquettes d'√©motions.\n",
    "\n",
    "## Formulation math√©matique\n",
    "\n",
    "### T√¢che ERC\n",
    "$$\n",
    "D = \\{(u_1, s_1), ..., (u_N, s_N)\\}, \\quad E = \\{e_1, ..., e_N\\}\n",
    "$$\n",
    "\n",
    "### Fusion croissante via vecteurs joints (JFM)\n",
    "- Initialisation :\n",
    "$$\n",
    "F^t = \\text{RoBERTa}(\\text{text}), \\quad F^m = \\text{ViT}(\\text{mel-spectrogram})\n",
    "$$\n",
    "- Pour chaque couche $$l$$ de fusion (JF block) :\n",
    "$$\n",
    "(F^l_{m\\rightarrow t} \\oplus v_j^l) = V\\text{Trans}^l(F^{l-1}_{m\\rightarrow t} \\oplus v_j^l) \\\\\n",
    "(F^{l+1}_{m\\rightarrow t} \\oplus \\bar{v}_j^l) = L\\text{Trans}^l(F^l_{t\\rightarrow m} \\oplus \\text{MLP}(v_j^l))\n",
    "$$\n",
    "(Et sym√©triquement pour $$t\\rightarrow m$$)\n",
    "\n",
    "- Fusion finale :\n",
    "$$\n",
    "F = [F^N_{m\\rightarrow t} \\oplus F^N_{t\\rightarrow m}]\n",
    "$$\n",
    "\n",
    "### Pr√©diction d'√©motion\n",
    "$$\n",
    "\\hat{y}_i = \\text{Softmax}(W F + b), \\quad \\mathcal{L}_{\\text{ERC}} = -\\sum_i r_i \\log(\\hat{r}_i)\n",
    "$$\n",
    "\n",
    "### Apprentissage contrastif inter-classe (ICL)\n",
    "$$\n",
    "\\mathcal{L}_{\\text{ICL}} = \\sum_{i \\in I} - \\frac{1}{|P(i)|} \\sum_{p \\in P(i)} \\log \\frac{\\exp(F_i \\cdot F_p / \\tau)}{\\sum_{j \\neq i} \\exp(F_i \\cdot F_j / \\tau)}\n",
    "$$\n",
    "avec $$P(i)$$ : les exemples positifs (m√™me √©motion), $$\\tau$$ : temp√©rature\n",
    "\n",
    "### Fonction de perte totale\n",
    "$$\n",
    "\\mathcal{L} = \\mathcal{L}_{\\text{ERC}} + \\lambda \\mathcal{L}_{\\text{ICL}}\n",
    "$$\n",
    "\n",
    "## Jeux de donn√©es et m√©triques\n",
    "- **IEMOCAP** : 7433 √©nonc√©s (6 √©motions)\n",
    "- **MELD** : 13708 √©nonc√©s (7 √©motions)\n",
    "- **M√©triques** : Accuracy et Weighted-F1 (W-F1)\n",
    "\n",
    "## Contribution originale\n",
    "- **Fusion multimodale par vecteurs joints (JFM)** avec Transformers\n",
    "- **Apprentissage contrastif supervis√©** inter-classe via √©tiquettes d'√©motions\n",
    "- R√©sultats SOTA ou tr√®s comp√©titifs sur IEMOCAP et MELD\n",
    "- Ablation fine : importance de chaque bloc fusion et vecteur joint quantifi√©e\n",
    "\n",
    "## Forces et limites\n",
    "\n",
    "### Forces\n",
    "- Maintien de l'identit√© modale pendant la fusion (pas de m√©lange brutal)\n",
    "- Gain net sur les classes rares gr√¢ce au ICL\n",
    "- Approche modulaire et facilement extensible (plus de blocs, plus de modalit√©s)\n",
    "- Performance robuste en mono-modal aussi (texte ou audio)\n",
    "\n",
    "### Limites\n",
    "- Le mod√®le ne traite pas le **contexte de conversation** (pas de mod√©lisation temporelle)\n",
    "- Pas de gestion des donn√©es **multi-locuteurs**\n",
    "- Sensible aux choix des hyperparam√®tres (longueur $$v_j$$, nombre de blocs $$N$$, $$\\tau$$)\n",
    "\n",
    "Ce mod√®le est une avanc√©e forte pour la fusion audio-texte avec un apprentissage supervis√© contrastif, mais il n'int√®gre pas encore la dynamique conversationnelle ou les aspects multi-locuteurs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae680c54",
   "metadata": {},
   "source": [
    "## **Comparaison des  methodes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed3cf7d",
   "metadata": {},
   "source": [
    "| Crit√®res                    | Article 1: BiStream        | Article 2: EnhancedERC     | Article 3: DialogueGCN      | Article 4: Hypergraphs       | Article 5: MMGCN            | Article 6: MultiEMO         | Article 7: ODE               | Article 8: RBA-GCN           | Article 9: TelME             |\n",
    "|----------------------------|----------------------------|----------------------------|-----------------------------|------------------------------|-----------------------------|-----------------------------|------------------------------|------------------------------|------------------------------|\n",
    "| **Mod√®le utilis√©**         | Bi-stream GNN (BiGMF)      | Cross-modal fusion + CL    | Graph CNN                  | Hypergraph GCN              | Deep GCN                   | Attention-based fusion     | GCN with ODE                 | Bilevel aggregation GCN      | Cross-modal distillation + fusion |\n",
    "| **Type de donn√©es**        | Texte, audio, visuel       | Texte, audio               | Texte                      | Texte, audio, visuel        | Texte, audio, visuel       | Texte, audio, visuel       | Texte, audio, visuel         | Texte, audio, visuel         | Texte, audio, visuel         |\n",
    "| **Gestion du contexte**    | Deux flux: intra/inter     | Contexte + vecteurs joints | Graphe dirig√©, locuteur    | Hypergraphes intra/inter    | Connexions graphes         | Context via attention      | Dynamique temporelle (ODE)  | Agr√©gation par clusters      | Fusion adaptative contextuelle |\n",
    "| **M√©thode d'entra√Ænement** | Supervis√©                  | Supervis√© + CL             | Supervis√©                  | Supervis√©                   | Supervis√©                  | Supervis√© + Focal loss     | Supervis√© + ODE             | Supervis√© avec BiAM          | Supervis√© + distillation     |\n",
    "| **Performance (pr√©cision)**| SOTA sur MELD/IEMOCAP     | Am√©lioration notable       | Meilleur que RNN/LSTM      | Sup√©rieure aux baselines    | Sup√©rieure aux SOTA       | Sup√©rieure aux SOTA       | Meilleure stabilit√©         | 2-5% > SOTA sur F1           | SOTA sur MELD                |\n",
    "| **√âvaluation sur (dataset)**| MELD, IEMOCAP             | MELD, IEMOCAP              | IEMOCAP, MELD              | MELD, IEMOCAP               | IEMOCAP, MELD              | IEMOCAP, MELD              | IEMOCAP, MELD                | IEMOCAP, MELD                | MELD, IEMOCAP                |\n",
    "| **Approche multimodale**   | S√©paration explicite       | Vecteurs conjoints         | N/A (textuel seulement)    | Hypergraphes multi-modal    | Graphe fusion modalit√©s    | Fusion via cross-attention | ODE sur graph multimodal    | Clustering modalit√©s         | Distillation crois√©e multimodale |\n",
    "| **Limites identifi√©es**    | Conflit modalit√©s          | Redondance information     | Pas d'info intermodale     | Complexit√© de graphe        | H√©t√©rog√©n√©it√© ignor√©e      | Difficult√© sur classes rares| Surapprentissage GCN        | Redondance info, bruit       | D√©pendance au mod√®le textuel |\n",
    "| **Applications principales**| ERC                       | ERC                        | ERC                        | ERC                         | ERC                        | ERC                        | ERC                          | ERC                          | ERC                          |\n",
    "| **Am√©liorations propos√©es**| Flux bi-modaux parall√®les  | Fusion adaptative + CL     | Graphe dirig√© locuteurs    | Fen√™tre acoustique dynamique| Fusion profonde            | SWFC + Soft-HGR            | MixHop + ODE                | Agr√©gation bilat√©rale        | Fusion modale hi√©rarchique  |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
