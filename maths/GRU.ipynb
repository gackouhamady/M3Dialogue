{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gated Recurrent Unit (GRU) Documentation\n",
    "\n",
    "## 1. Introduction\n",
    "A Gated Recurrent Unit (GRU) is a type of recurrent neural network (RNN) introduced by Cho et al. in 2014 as a simplified version of the Long Short-Term Memory (LSTM) network. GRUs are effective in capturing sequential dependencies while addressing the vanishing gradient problem that standard RNNs face.\n",
    "\n",
    "Reference paper: [Cho et al., 2014 - Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078)\n",
    "\n",
    "## 2. Mathematical Formulation\n",
    "A GRU consists of two main gates:\n",
    "- **Update Gate**: Decides how much past information should be carried forward.\n",
    "- **Reset Gate**: Determines how much of the past information should be forgotten.\n",
    "\n",
    "Let:\n",
    "- \\( x_t \\) be the input vector at time step \\( t \\)\n",
    "- \\( h_t \\) be the hidden state at time step \\( t \\)\n",
    "- \\( W_z, W_r, W_h \\) be weight matrices for the update, reset, and candidate hidden state\n",
    "- \\( U_z, U_r, U_h \\) be recurrent weight matrices\n",
    "- \\( b_z, b_r, b_h \\) be biases\n",
    "- \\( \\sigma \\) be the sigmoid activation function\n",
    "- \\( \\odot \\) be element-wise multiplication\n",
    "- \\( \\tanh \\) be the hyperbolic tangent activation function\n",
    "\n",
    "The GRU update equations are:\n",
    "\n",
    "### 2.1. Reset Gate\n",
    "$$ [\n",
    "r_t = \\sigma(W_r x_t + U_r h_{t-1} + b_r)\n",
    "] $$\n",
    "\n",
    "### 2.2. Update Gate\n",
    "$$[\n",
    "z_t = \\sigma(W_z x_t + U_z h_{t-1} + b_z)\n",
    "]$$\n",
    "\n",
    "### 2.3. Candidate Hidden State\n",
    "$$[\n",
    "\\tilde{h}_t = \\tanh(W_h x_t + U_h (r_t \\odot h_{t-1}) + b_h)\n",
    "]$$\n",
    "\n",
    "### 2.4. Final Hidden State Update\n",
    "$$[\n",
    "h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t\n",
    "]$$\n",
    "\n",
    "## 3. Explanation\n",
    "- The **reset gate** \\( r_t \\) decides whether to ignore past hidden states.\n",
    "- The **update gate** \\( z_t \\) determines how much of the past hidden state should be retained.\n",
    "- The **candidate hidden state** \\( \\tilde{h}_t \\) computes a new state based on the reset hidden state.\n",
    "- The **final hidden state** \\( h_t \\) is a linear interpolation of the past hidden state and the candidate hidden state.\n",
    "\n",
    "## 4. Advantages of GRU\n",
    "- **Fewer parameters** than LSTMs (since it lacks an explicit cell state)\n",
    "- **Computationally efficient**\n",
    "- **Avoids vanishing gradient issues** better than vanilla RNNs\n",
    "- **Performs well in sequential tasks** (e.g., speech recognition, NLP, time-series prediction)\n",
    "\n",
    "## 5. Reference Paper\n",
    "Cho, Kyunghyun, et al. *\"Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation.\"* arXiv preprint arXiv:1406.1078 (2014).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
