{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Attention Networks (GAT): Mathematical Background\n",
    "\n",
    "This document provides a mathematical overview of Graph Attention Networks (GAT), as introduced by Veličković et al. (2018). It includes key equations and the official paper link.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Mathematical Formulation](#mathematical-formulation)\n",
    "3. [Attention Mechanism](#attention-mechanism)\n",
    "4. [Multi-Head Attention](#multi-head-attention)\n",
    "5. [Official Paper](#official-paper)\n",
    "6. [Conclusion](#conclusion)\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Graph Attention Networks (GAT) leverage attention mechanisms to learn on graph-structured data by assigning different weights to neighbor nodes.\n",
    "\n",
    "Les Graph Attention Networks (GAT) utilisent des mécanismes d'attention pour apprendre à partir de données structurées en graphe en attribuant des poids différents aux nœuds voisins.\n",
    "\n",
    "---\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "Given a graph with $$ \\N  $$ nodes and node features $$  \\{ \\mathbf{h}_1, \\mathbf{h}_2, \\dots, \\mathbf{h}_N \\} $$ where each $$ \\mathbf{h}_i \\in \\mathbb{R}^F $$, the first step is to apply a shared linear transformation:\n",
    "\n",
    "$$\n",
    "\\mathbf{h}'_i = W \\mathbf{h}_i,\\quad W \\in \\mathbb{R}^{F' \\times F}\n",
    "$$\n",
    "\n",
    "This maps the features into a new \\( F' \\)-dimensional space.\n",
    "\n",
    "Étant donné un graphe avec $$ ( \\N ) $$ nœuds et des caractéristiques $$ \\{ \\mathbf{h}_1, \\mathbf{h}_2, \\dots, \\mathbf{h}_N \\} $$ où chaque $$ \\mathbf{h}_i \\in \\mathbb{R}^F $$, la première étape consiste à appliquer une transformation linéaire partagée :\n",
    "\n",
    "$$\n",
    "\\mathbf{h}'_i = W \\mathbf{h}_i,\\quad W \\in \\mathbb{R}^{F' \\times F}\n",
    "$$\n",
    "\n",
    "Cela permet de projeter les caractéristiques dans un nouvel espace de dimension \\( F' \\).\n",
    "\n",
    "---\n",
    "\n",
    "## Attention Mechanism\n",
    "\n",
    "For each node \\( i \\) and its neighbor \\( j \\) (i.e., \\( j \\in \\mathcal{N}_i \\)), compute the unnormalized attention coefficient:\n",
    "\n",
    "$$\n",
    " e_{ij} = \\text{LeakyReLU}\\left( \\mathbf{a}^T \\left[ \\mathbf{h}'_i \\, \\Vert \\, \\mathbf{h}'_j \\right] \\right)\n",
    "$$\n",
    "\n",
    "where \\( \\mathbf{a} \\in \\mathbb{R}^{2F'} \\) is a learnable weight vector and \\( \\Vert \\) denotes concatenation.\n",
    "\n",
    "Pour chaque nœud \\( i \\) et son voisin \\( j \\) (c'est-à-dire \\( j \\in \\mathcal{N}_i \\)), on calcule le coefficient d'attention non normalisé :\n",
    "\n",
    "$$\n",
    " e_{ij} = \\text{LeakyReLU}\\left( \\mathbf{a}^T \\left[ \\mathbf{h}'_i \\, \\Vert \\, \\mathbf{h}'_j \\right] \\right)\n",
    "$$\n",
    "\n",
    "où \\( \\mathbf{a} \\in \\mathbb{R}^{2F'} \\) est un vecteur de poids appris et \\( \\Vert \\) désigne la concaténation.\n",
    "\n",
    "These coefficients are then normalized using the softmax function:\n",
    "\n",
    "$$\n",
    "\\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}_i} \\exp(e_{ik})}\n",
    "$$\n",
    "\n",
    "Ces coefficients sont ensuite normalisés avec la fonction softmax :\n",
    "\n",
    "$$\n",
    "\\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}_i} \\exp(e_{ik})}\n",
    "$$\n",
    "\n",
    "The output feature for node \\( i \\) is computed as:\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_i^* = \\sigma \\left( \\sum_{j \\in \\mathcal{N}_i} \\alpha_{ij} \\mathbf{h}'_j \\right)\n",
    "$$\n",
    "\n",
    "where \\( \\sigma \\) is a non-linear activation function.\n",
    "\n",
    "La caractéristique de sortie pour le nœud \\( i \\) est calculée comme suit :\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_i^* = \\sigma \\left( \\sum_{j \\in \\mathcal{N}_i} \\alpha_{ij} \\mathbf{h}'_j \\right)\n",
    "$$\n",
    "\n",
    "où \\( \\sigma \\) est une fonction d'activation non linéaire.\n",
    "\n",
    "---\n",
    "\n",
    "## Multi-Head Attention\n",
    "\n",
    "To improve model capacity and stability, GAT uses multi-head attention:\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_i^* = \\mathbin\\Vert_{k=1}^K \\sigma \\left( \\sum_{j \\in \\mathcal{N}_i} \\alpha_{ij}^{(k)} W^{(k)} \\mathbf{h}_j \\right)\n",
    "$$\n",
    "\n",
    "Each head \\( k \\) computes its own set of attention coefficients \\( \\alpha_{ij}^{(k)} \\) and weight matrix \\( W^{(k)} \\), and the results are concatenated or averaged.\n",
    "\n",
    "Pour améliorer la capacité du modèle et sa stabilité, les GAT utilisent l'attention multi-têtes :\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_i^* = \\mathbin\\Vert_{k=1}^K \\sigma \\left( \\sum_{j \\in \\mathcal{N}_i} \\alpha_{ij}^{(k)} W^{(k)} \\mathbf{h}_j \\right)\n",
    "$$\n",
    "\n",
    "Chaque tête \\( k \\) calcule son propre ensemble de coefficients d'attention \\( \\alpha_{ij}^{(k)} \\) et sa matrice de poids \\( W^{(k)} \\), et les résultats sont concaténés ou moyennés.\n",
    "\n",
    "---\n",
    "\n",
    "## Official Paper\n",
    "\n",
    "For more detailed information, please refer to the official paper:\n",
    "\n",
    "Pour plus d'informations détaillées, veuillez consulter l'article officiel :\n",
    "\n",
    "**Graph Attention Networks**  \n",
    "[Veličković et al., 2018](https://arxiv.org/abs/1710.10903)\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "GAT offers a powerful framework to integrate both node features and graph structure by learning the importance of neighboring nodes via attention. This leads to improved performance on a variety of graph-based tasks.\n",
    "\n",
    "Les GAT offrent un cadre puissant pour intégrer à la fois les caractéristiques des nœuds et la structure du graphe en apprenant l'importance relative des nœuds voisins via un mécanisme d'attention. Cela conduit à de meilleures performances sur diverses tâches liées aux graphes."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
