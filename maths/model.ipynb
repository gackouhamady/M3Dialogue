{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DialogueGCNModel Documentation\n",
    "\n",
    "This document provides an overview of the `DialogueGCNModel` and its components, explaining each class and how they contribute to the model's functionality. The model is designed for emotion recognition in conversation using deep learning techniques like RNNs, GCNs, and attention mechanisms.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Overview](#overview)\n",
    "2. [Mathematical Background](#mathematical-background)\n",
    "3. [Classes and Components](#classes-and-components)\n",
    "    - [MaskedNLLLoss](#maskednllloss)\n",
    "    - [SimpleAttention](#simpleattention)\n",
    "    - [MatchingAttention](#matchingattention)\n",
    "    - [DialogueRNNCell](#dialoguernncell)\n",
    "    - [DialogueRNN](#dialoguernn)\n",
    "    - [MaskedEdgeAttention](#maskededgeattention)\n",
    "    - [GraphNetwork](#graphnetwork)\n",
    "    - [DialogueGCNModel](#dialoguegcnmodel)\n",
    "4. [Usage](#usage)\n",
    "5. [References](#references)\n",
    "\n",
    "## Overview\n",
    "\n",
    "The `DialogueGCNModel` leverages a combination of Graph Neural Networks (GNNs), Recurrent Neural Networks (RNNs), and Attention mechanisms to model dialogues for emotion recognition. The model processes conversational data to predict the emotional state of the speakers at each step.\n",
    "\n",
    "---\n",
    "\n",
    "## Mathematical Background\n",
    "\n",
    "### 1. **Recurrent Neural Networks (RNNs)**\n",
    "\n",
    "RNNs are used to model sequences by maintaining a hidden state at each time step that captures information from previous inputs.\n",
    "\n",
    "Mathematically, at each time step $$( t )$$, the hidden state $$( h_t )$$ is updated as follows:\n",
    "\n",
    "$$\n",
    "h_t = f(W_{xh} x_t + W_{hh} h_{t-1} + b_h)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( h_t \\) is the hidden state at time \\( t \\),\n",
    "- \\( x_t \\) is the input at time \\( t \\),\n",
    "- \\( W_{xh}, W_{hh} \\) are weights, and\n",
    "- \\( f \\) is the activation function (usually tanh or ReLU).\n",
    "\n",
    "RNNs are widely used for processing sequential data like dialogue because they capture temporal dependencies in a sequence.\n",
    "\n",
    "Official Link: [RNNs Overview](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "### 2. **Graph Convolutional Networks (GCNs)**\n",
    "\n",
    "GCNs apply convolutions on graph-structured data. The graph structure allows the model to capture dependencies between entities, such as words or tokens in dialogue.\n",
    "\n",
    "The graph convolution operation can be defined as:\n",
    "\n",
    "$$\n",
    "h' = \\sigma( \\hat{A} X W )\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( hat{A} \\) is the normalized adjacency matrix with added self-connections,\n",
    "- \\( X \\) is the feature matrix of nodes,\n",
    "- \\( W \\) is the learnable weight matrix, and\n",
    "- \\( sigma \\) is a nonlinear activation function (e.g., ReLU).\n",
    "\n",
    "GCNs allow the model to use information from neighboring nodes, which is essential for dialogue context modeling.\n",
    "\n",
    "Official Link: [GCN Paper](https://arxiv.org/abs/1609.02907)\n",
    "\n",
    "### 3. **Attention Mechanisms**\n",
    "\n",
    "Attention mechanisms allow models to focus on different parts of the input sequence at each step, which is crucial for tasks like translation and dialogue modeling.\n",
    "\n",
    "The basic attention mechanism computes a weighted sum of input vectors based on learned attention scores:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}( \\frac{Q K^T}{\\sqrt{d_k}} ) V\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( Q \\) is the query,\n",
    "- \\( K \\) is the key,\n",
    "- \\( V \\) is the value, and\n",
    "- \\( d_k \\) is the dimension of the key vectors.\n",
    "\n",
    "Attention helps the model selectively focus on relevant tokens in the dialogue.\n",
    "\n",
    "Official Link: [Attention is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "---\n",
    "\n",
    "## Classes and Components\n",
    "\n",
    "### MaskedNLLLoss\n",
    "\n",
    "`MaskedNLLLoss` is a custom loss function that computes the negative log-likelihood loss while considering a mask to ignore certain tokens in the input sequence.\n",
    "\n",
    "#### Mathematical Formulation:\n",
    "The masked negative log-likelihood loss is calculated as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{NLL} = - \\sum_{i=1}^{N} \\mathbb{I}_{mask_i} \\log p(y_i | x)\n",
    "$$\n",
    "\n",
    "Where: $$  \\mathbb{I}_{mask_i} $$ is the mask indicator,\n",
    "- \\( p(y_i | x) \\) is the probability of the true label \\( y_i \\) given input \\( x \\),\n",
    "- \\( N \\) is the number of tokens.\n",
    "\n",
    "### SimpleAttention\n",
    "\n",
    "`SimpleAttention` applies a basic attention mechanism to the input sequence.\n",
    "\n",
    "#### Mathematical Formulation:\n",
    "For a sequence of inputs \\( X \\), the attention score is computed as:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(X) = \\text{softmax}(W X)\n",
    "$$\n",
    "\n",
    "Where \\( W \\) is a learnable weight matrix.\n",
    "\n",
    "### MatchingAttention\n",
    "\n",
    "`MatchingAttention` is a more complex attention mechanism that computes attention between two sequences using different attention types.\n",
    "\n",
    "#### Mathematical Formulation:\n",
    "The attention weight is computed using the similarity between the query \\( Q \\) and key \\( K \\) vectors:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K) = \\frac{\\exp(Q K^T)}{\\sum_{i=1}^{N} \\exp(Q K_i^T)}\n",
    "$$\n",
    "\n",
    "### DialogueRNNCell\n",
    "\n",
    "`DialogueRNNCell` defines the core RNN cell used for processing each step in the dialogue. It incorporates multiple GRU cells and attention mechanisms.\n",
    "\n",
    "#### Mathematical Formulation:\n",
    "The GRU (Gated Recurrent Unit) update rule is given by:\n",
    "\n",
    "$$\n",
    "z_t = \\sigma(W_z x_t + U_z h_{t-1} + b_z)\n",
    "$$\n",
    "$$\n",
    "r_t = \\sigma(W_r x_t + U_r h_{t-1} + b_r)\n",
    "$$\n",
    "$$\n",
    "\\tilde{h}_t = \\tanh(W_h x_t + U_h (r_t \\circ h_{t-1}) + b_h)\n",
    "$$\n",
    "$$\n",
    "h_t = (1 - z_t) \\circ h_{t-1} + z_t \\circ \\tilde{h}_t\n",
    "$$\n",
    "\n",
    "Where \\( z_t \\) is the update gate, \\( r_t \\) is the reset gate, and $$ ( \\circ ) $$ denotes element-wise multiplication.\n",
    "\n",
    "### DialogueRNN\n",
    "\n",
    "`DialogueRNN` utilizes multiple `DialogueRNNCell` layers to process an entire dialogue sequence.\n",
    "\n",
    "### MaskedEdgeAttention\n",
    "\n",
    "`MaskedEdgeAttention` applies an attention mechanism to a graph's edges, considering specific relations between dialogue tokens.\n",
    "\n",
    "#### Mathematical Formulation:\n",
    "The edge attention mechanism is defined as:\n",
    "\n",
    "$$\n",
    "\\alpha_{ij} = \\text{softmax}(W \\cdot (h_i || h_j))\n",
    "$$\n",
    "\n",
    "Where \\( h_i \\) and \\( h_j \\) are the node representations, \\( || \\) denotes concatenation, and \\( W \\) is a learnable weight matrix.\n",
    "\n",
    "### GraphNetwork\n",
    "\n",
    "`GraphNetwork` combines GCNs and attention mechanisms to process the dialogue graph.\n",
    "\n",
    "#### Mathematical Formulation:\n",
    "The graph convolution operation is applied to the nodes and edges as follows:\n",
    "\n",
    "$$\n",
    "h' = \\sigma( \\hat{A} X W)\n",
    "$$\n",
    "\n",
    "Where $$ ( \\hat{A} ) $$ is the normalized adjacency matrix, and $$ ( X )$$ is the node feature matrix.\n",
    "\n",
    "---\n",
    "\n",
    "## DialogueGCNModel\n",
    "\n",
    "`DialogueGCNModel` combines the various components (RNN, GCN, attention) for emotion recognition in conversations.\n",
    "\n",
    "### Mathematical Formulation:\n",
    "\n",
    "Given input sequence $$( X )$$, edge information $$( A )$$, and attention weights $$( \\alpha )$$, the output of the model is computed as:\n",
    "\n",
    "$$\n",
    "y = \\text{softmax}(W_{out} h_{\\text{final}})\n",
    "$$\n",
    "\n",
    "Where $$ ( h_{\\text{final}} )$$ is the final hidden state after processing through the graph and attention layers.\n",
    "\n",
    "## Usage\n",
    "\n",
    "To use the `DialogueGCNModel`, you will need to instantiate it and provide the required inputs, such as features, masks, and edge data.\n",
    "\n",
    "```python\n",
    "# Example usage\n",
    "model = DialogueGCNModel(base_model='DialogRNN', D_m=256, D_g=128, D_p=64, D_e=32, D_h=64, D_a=100, \n",
    "                         graph_hidden_size=128, n_speakers=2, max_seq_len=50, window_past=5, window_future=5)\n",
    "output = model(features, edge_index, edge_norm, edge_type, seq_lengths, umask, nodal_attn=True, avec=False) \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
