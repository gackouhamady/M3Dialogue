{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematical Concepts Explained:\n",
    "\n",
    "## 1. Data Preprocessing and Loading:\n",
    "### DataLoader & Sampler:\n",
    "\n",
    "The dataset is split into training, validation, and test sets using the SubsetRandomSampler. The training and validation sets are sampled with a certain fraction (default valid=0.1 means 10% for validation).\n",
    "\n",
    "DataLoader handles batching and shuffling of the data.\n",
    "\n",
    "**Mathematics:** No heavy mathematical computation here. This part simply arranges data for feeding into models.\n",
    "\n",
    "**Documentation:** [PyTorch DataLoader](https://pytorch.org/docs/stable/data.html)\n",
    "\n",
    "## 2. Neural Networks and Model Architectures:\n",
    "### Recurrent Models (RNN, GRU, LSTM): \n",
    "These are models designed for sequence processing. LSTM and GRU are more advanced forms of RNNs designed to mitigate issues like vanishing gradients in long sequences.\n",
    "\n",
    "### Graph Neural Networks (GNN): \n",
    "The DialogueGCNModel extends the traditional neural network model by adding graph-based learning, using graph convolutional layers to model relationships between sequential data in conversation (e.g., past and future utterances).\n",
    "\n",
    "**Mathematics:**\n",
    "\n",
    "- **RNN/GRU/LSTM:** These models use backpropagation through time (BPTT) for training, where gradients are calculated recursively over time steps.\n",
    "- **GNN:** Graph Convolutional Networks (GCN) extend neural networks to graph-structured data. Each node in the graph is updated based on its neighbors.\n",
    "\n",
    "**Documentation:**\n",
    "\n",
    "- [RNN, LSTM, GRU in PyTorch](https://pytorch.org/docs/stable/nn.html#recurrent-layers)\n",
    "- [Graph Convolution Networks (GCN)](https://pytorch-geometric.readthedocs.io/en/latest/)\n",
    "\n",
    "## 3. Loss Function:\n",
    "### Cross-Entropy Loss (NLLLoss):\n",
    "\n",
    "The loss function used here is a Negative Log Likelihood Loss (NLLLoss), commonly used for multi-class classification tasks.\n",
    "\n",
    "It calculates the logarithm of the predicted probabilities for the correct class and penalizes wrong predictions. This is a standard loss function for multi-class classification in neural networks.\n",
    "\n",
    "### Masked Loss: \n",
    "For sequential data (such as text or speech), the MaskedNLLLoss is used to ignore padding tokens when calculating the loss.\n",
    "\n",
    "**Mathematics:**\n",
    "\n",
    "**NLLLoss:**\n",
    "$$[\n",
    "Loss(x, y) = - \\log(p_y)\n",
    "]$$\n",
    "where \\(p_y\\) is the probability of the correct class \\(y\\) predicted by the model.\n",
    "\n",
    "**Documentation:** [PyTorch NLLLoss](https://pytorch.org/docs/stable/nn.html#nllloss)\n",
    "\n",
    "## 4. Optimization:\n",
    "### Adam Optimizer:\n",
    "\n",
    "The model's parameters are updated using the Adam optimizer, which is a variant of gradient descent that adapts the learning rate for each parameter based on first and second moments of the gradients.\n",
    "\n",
    "**Mathematics:**\n",
    "\n",
    "Adam optimization uses the following formula to update parameters $$(\\theta)$$:\n",
    "$$[\n",
    "\\theta_{t+1} = \\theta_t - \\eta \\cdot \\frac{v_t}{\\sqrt{m_t} + \\epsilon}\n",
    "]$$\n",
    "where:\n",
    "\n",
    "- \\(m_t\\) is the first moment (mean of gradients),\n",
    "- \\(v_t\\) is the second moment (variance of gradients),\n",
    "- \\(\\eta\\) is the learning rate, and\n",
    "- \\(\\epsilon\\) is a small constant to avoid division by zero.\n",
    "\n",
    "**Documentation:** [Adam Optimizer in PyTorch](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam)\n",
    "\n",
    "## 5. Evaluation Metrics:\n",
    "### Accuracy, F1-Score: \n",
    "These metrics are calculated to evaluate the performance of the model.\n",
    "\n",
    "### Confusion Matrix: \n",
    "Used to assess how well the model classifies the different emotion labels.\n",
    "\n",
    "### Weighted F1-Score: \n",
    "The F1-score is calculated using the weighted average, which accounts for class imbalances.\n",
    "\n",
    "**Mathematics:**\n",
    "\n",
    "- **Accuracy:**\n",
    "$$[\n",
    "Accuracy = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}\n",
    "]$$\n",
    "- **F1-Score:**\n",
    "$$[\n",
    "F1 \\text{-score} = \\frac{2 \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "]$$\n",
    "where:\n",
    "\n",
    "- **Precision** = $$(\\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}})$$\n",
    "- **Recall** = $$(\\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}})$$\n",
    "\n",
    "**Documentation:**\n",
    "\n",
    "- [Classification Report in scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)\n",
    "- [F1-Score Calculation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)\n",
    "\n",
    "## 6. Model Checkpointing:\n",
    "After each epoch, the model's parameters are saved using `torch.save`. This allows saving the state of the model at the best epoch, which can be loaded for later evaluation or deployment.\n",
    "\n",
    "**Mathematics:** No direct mathematical operations in saving models, but this is critical for training stability and reusability.\n",
    "\n",
    "**Documentation:** [Saving and Loading Models in PyTorch](https://pytorch.org/tutorials/beginner/saving_loading_models.html)\n",
    "\n",
    "## Step-by-Step Breakdown:\n",
    "### Data Preparation:\n",
    "Dataset is loaded, split, and batched using PyTorchâ€™s DataLoader and SubsetRandomSampler.\n",
    "\n",
    "### Model Selection:\n",
    "The base model (LSTM, GRU, or GCN) is selected based on the argument passed. Each model uses specific layers, such as LSTM cells, GRU cells, or graph convolutions.\n",
    "\n",
    "### Training:\n",
    "The model is trained over multiple epochs. During each epoch, gradients are calculated and parameters are updated using the Adam optimizer.\n",
    "\n",
    "### Evaluation:\n",
    "After training, the model is evaluated on the validation and test sets, and metrics like accuracy, F1-score, and confusion matrix are calculated.\n",
    "\n",
    "### Logging:\n",
    "Tensorboard is used for visualizing training and evaluation metrics.\n",
    "\n",
    "### Model Checkpointing:\n",
    "After each epoch, the model is saved to prevent data loss.\n",
    "\n",
    "## Official Documentation:\n",
    "- [PyTorch Overview](https://pytorch.org/docs/stable/index.html)\n",
    "- [Adam Optimizer](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam)\n",
    "- [NLLLoss](https://pytorch.org/docs/stable/nn.html#nllloss)\n",
    "- [DataLoader](https://pytorch.org/docs/stable/data.html)\n",
    "- [Graph Neural Networks (PyTorch Geometric)](https://pytorch-geometric.readthedocs.io/en/latest/)\n",
    "- [Scikit-learn Metrics](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)\n",
    "\n",
    "This explanation covers the key mathematical operations and how they are implemented in the provided code. For more details on each part, refer to the links above for the official documentation of each method or concept.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
